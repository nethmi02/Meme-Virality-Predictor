{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import os  # For file checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading JSON file...\")\n",
    "with open('memes.json', 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "\n",
    "print(\"Converting '_default' to DataFrame...\")\n",
    "memes_df = pd.DataFrame(data['_default'].values)\n",
    "if len(memes_df.columns) == 1 and isinstance(memes_df.iloc[0, 0], dict):\n",
    "    memes_df = pd.DataFrame(list(memes_df[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data into a dataframe\n",
    "memes_df = memes_df[['title', 'ups', 'created_utc']].dropna()\n",
    "print(\"Initial DataFrame:\")\n",
    "print(memes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the threshold for virality as the 75th percentile of upvotes (top 25% are considered viral)\n",
    "threshold = memes_df['ups'].quantile(0.75)  \n",
    "print(f\"Threshold for virality: {threshold} upvotes\")\n",
    "memes_df['is_viral'] = (memes_df['ups'] >= threshold).astype(int)\n",
    "memes_df = memes_df.drop(columns=['ups'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of words and characters in the title\n",
    "memes_df['word_count'] = memes_df['title'].apply(lambda x: len(str(x).split()))\n",
    "memes_df['char_count'] = memes_df['title'].apply(len)\n",
    "memes_df['sentiment'] = memes_df['title'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "print(\"DataFrame with Features:\")\n",
    "print(memes_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of emojis in the text using unicode ranges\n",
    "def count_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "                               u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # Enclosed characters \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return len(emoji_pattern.findall(str(text)))\n",
    "\n",
    "memes_df['emoji_count'] = memes_df['title'].apply(count_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posting hour from UTC timestamp for timing analysis\n",
    "def get_hour(timestamp):\n",
    "    return datetime.utcfromtimestamp(int(timestamp)).hour\n",
    "\n",
    "memes_df['post_hour'] = memes_df['created_utc'].apply(get_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count uppercase letters in the title for emphasis analysis\n",
    "def count_caps(text):\n",
    "    return sum(1 for char in str(text) if char.isupper())\n",
    "\n",
    "memes_df['cap_count'] = memes_df['title'].apply(count_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the dataframe with all features\n",
    "print(\"DataFrame with all features:\")\n",
    "print(memes_df.head())\n",
    "\n",
    "\n",
    "#split the data into training and testing sets\n",
    "\n",
    "X = memes_df[['word_count', 'char_count', 'sentiment', 'emoji_count', 'post_hour', 'cap_count']]\n",
    "y = memes_df['is_viral']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier on the scaled feature data\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model and scaler\n",
    "joblib.dump(model, 'meme_predictor_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(\"Model and scaler saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the virality of a meme\n",
    "def predict_meme_virality(title, post_hour=None):\n",
    "    word_count = len(title.split())\n",
    "    char_count = len(title)\n",
    "    sentiment = TextBlob(title).sentiment.polarity\n",
    "    emoji_count = count_emojis(title)\n",
    "    cap_count = count_caps(title)\n",
    "    post_hour = post_hour if post_hour is not None else 12  \n",
    "    \n",
    "    new_meme = pd.DataFrame({\n",
    "        'word_count': [word_count],\n",
    "        'char_count': [char_count],\n",
    "        'sentiment': [sentiment],\n",
    "        'emoji_count': [emoji_count],\n",
    "        'post_hour': [post_hour],\n",
    "        'cap_count': [cap_count]\n",
    "    })\n",
    "    new_meme_scaled = scaler.transform(new_meme)\n",
    "    prediction = model.predict(new_meme_scaled)[0]\n",
    "    prob = model.predict_proba(new_meme_scaled)[0][1]\n",
    "    return \"Viral\" if prediction == 1 else \"Not Viral\", prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load or train the model \n",
    "def load_or_train_model():\n",
    "    model_file = 'meme_predictor_model.pkl'\n",
    "    scaler_file = 'scaler.pkl'\n",
    "    \n",
    "    if os.path.exists(model_file) and os.path.exists(scaler_file):\n",
    "        model = joblib.load(model_file)\n",
    "        scaler = joblib.load(scaler_file)\n",
    "        print(\"Loaded pre-trained model and scaler.\")  \n",
    "    else:\n",
    "        print(\"Loading JSON file...\")\n",
    "        with open('memes.json', 'r') as file:\n",
    "            data = pd.read_json(file)\n",
    "        \n",
    "        print(\"Converting '_default' to DataFrame...\")\n",
    "        memes_df = pd.DataFrame(data['_default'].values)\n",
    "        if len(memes_df.columns) == 1 and isinstance(memes_df.iloc[0, 0], dict):\n",
    "            memes_df = pd.DataFrame(list(memes_df[0]))\n",
    "        \n",
    "        memes_df = memes_df[['title', 'ups', 'created_utc']].dropna()\n",
    "        print(\"Initial DataFrame:\")\n",
    "        print(memes_df.head())\n",
    "        \n",
    "        threshold = memes_df['ups'].quantile(0.75)\n",
    "        print(f\"Threshold for virality: {threshold} upvotes\")\n",
    "        memes_df['is_viral'] = (memes_df['ups'] >= threshold).astype(int)\n",
    "        memes_df = memes_df.drop(columns=['ups'])\n",
    "        \n",
    "        memes_df['word_count'] = memes_df['title'].apply(lambda x: len(str(x).split()))\n",
    "        memes_df['char_count'] = memes_df['title'].apply(len)\n",
    "        memes_df['sentiment'] = memes_df['title'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "        memes_df['emoji_count'] = memes_df['title'].apply(count_emojis)\n",
    "        memes_df['post_hour'] = memes_df['created_utc'].apply(get_hour)\n",
    "        memes_df['cap_count'] = memes_df['title'].apply(count_caps)\n",
    "        \n",
    "        print(\"DataFrame with all features:\")\n",
    "        print(memes_df.head())\n",
    "        \n",
    "        X = memes_df[['word_count', 'char_count', 'sentiment', 'emoji_count', 'post_hour', 'cap_count']]\n",
    "        y = memes_df['is_viral']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        joblib.dump(model, model_file)\n",
    "        joblib.dump(scaler, scaler_file)\n",
    "        print(\"Model and scaler saved!\")\n",
    "    \n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the virality of a meme\n",
    "def predict_meme_virality(title, post_hour=None, model=None, scaler=None):\n",
    "    word_count = len(title.split())\n",
    "    char_count = len(title)\n",
    "    sentiment = TextBlob(title).sentiment.polarity\n",
    "    emoji_count = count_emojis(title)\n",
    "    cap_count = count_caps(title)\n",
    "    post_hour = post_hour if post_hour is not None else 12  \n",
    "    \n",
    "    new_meme = pd.DataFrame({\n",
    "        'word_count': [word_count],\n",
    "        'char_count': [char_count],\n",
    "        'sentiment': [sentiment],\n",
    "        'emoji_count': [emoji_count],\n",
    "        'post_hour': [post_hour],\n",
    "        'cap_count': [cap_count]\n",
    "    })\n",
    "    new_meme_scaled = scaler.transform(new_meme)\n",
    "    prediction = model.predict(new_meme_scaled)[0]\n",
    "    prob = model.predict_proba(new_meme_scaled)[0][1]\n",
    "    \n",
    "    result = \"Viral\" if prediction == 1 else \"Not Viral\"\n",
    "    suggestions = []\n",
    "    \n",
    "    # If not viral, provide suggestions based on feature analysis\n",
    "    if prediction == 0:\n",
    "        if word_count < 5:\n",
    "            suggestions.append(\"Try making the title longer (5+ words) for more context or humor.\")\n",
    "        if emoji_count == 0:\n",
    "            suggestions.append(\"Add some emojis (😄👍) to make it more engaging!\")\n",
    "        if cap_count < 2:\n",
    "            suggestions.append(\"Use more CAPS for emphasis (e.g., 'THIS IS FUNNY').\")\n",
    "        if sentiment <= 0:\n",
    "            suggestions.append(\"Make it more positive or humorous for better appeal.\")\n",
    "        if post_hour not in range(17, 22):  \n",
    "            suggestions.append(\"Post between 5 PM and 10 PM for higher engagement.\")\n",
    "        if suggestions:\n",
    "            st.write(\"**Suggestions to improve virality:**\")\n",
    "            for suggestion in suggestions:\n",
    "                st.write(f\"- {suggestion}\")  \n",
    "    \n",
    "    return result, prob, suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streamlit app \n",
    "def main():\n",
    "    st.title(\"Meme Virality Predictor\")  \n",
    "    st.write(\"Enter a meme and posting hour to predict if it’ll go viral!\")  \n",
    "    \n",
    "    # Load or train model\n",
    "    model, scaler = load_or_train_model()\n",
    "    \n",
    "    # Streamlit: User input fields\n",
    "    title = st.text_area(\"\")  \n",
    "    post_hour = st.slider(\"Posting Hour (0-23)\", 0, 23, 12) \n",
    "    \n",
    "   \n",
    "    if st.button(\"Predict\"):  # Button to trigger prediction\n",
    "        result, prob, suggestions = predict_meme_virality(title, post_hour, model, scaler)\n",
    "        st.write(f\"**Prediction**: {result}\")  \n",
    "        st.write(f\"**Probability of being viral**: {prob:.2%}\") \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
